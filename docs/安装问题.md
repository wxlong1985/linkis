1. 根据官方进行编绎 参考链接https://linkis.apache.org/zh-CN/docs/1.4.0/development/build
2. 修改bin/start-all.sh.sh 的 startDssWeb 的nginx启动为 ngins -s reload  以免影响整个nginx
2. conf/db.sh config.sh
3. 执行bin/install.sh
3. dss/conf/dss-server.properties
8. linkis/conf/config.sh db.sh
4. linkis/conf/linkis.properties
```shell
wds.linkis.bml.hdfs.prefix=hdfs:///opt/module/dss_linkis/tmp/linkis
该配置默认以hdfs://开头 务必要去除
```
5. linkis/conf/linkis-env.sh
6. linkis/conf/linkis-mg-gateway.properties
7. linkis/conf/linkis-ps-publicservice.properties



### 出现问题及对应解决方式
1. HADOOP_CONFIG_DIR没找到
HIVE_CONFIG_DIR没找到
需要配置环境变量如下，这里不明白，为什么
export HADOOP_CONF_DIR=/opt/module/hadoop-3.2.4/etc/hadoop/
export HIVE_CONF_DIR=/opt/module/apache-hive-3.1.2-bin/config/
判断代码在org.apache.linkis.ecm.core.launch.ProcessEngineConnLaunch.launch这个方法里面
CommonVars读取变量没找到。CommonVars源码有从linkis.properties里面去读取。该文件也有配置。为什么读取不到？


2. 引擎目录带以v开头的支持linkis1.4.0
org.apache.linkis.engineplugin.server.service.DefaultEngineConnResourceService.getEngineConnBMLResources
linkis.engineconn.bml.version.may.with.prefix  配置为false  或者修改代码

3. 自编绎引擎，在执行工作流时找不到类
任务执行原理是通过在appcom目录生成相应的sh文件、依赖、配置。启动时，通过执行sh文件来实现。执行过程当中。
会新起微服务linkis-cg-engineconn。引用的jar，主要引用${link_dir}/lib/linkis-commons/public-module.
需要手工往对应的目录加入新引入的jar。为什么不是自动从节点对应的引擎目录拷贝对应的jar到工作流目录？

4. yarn资源支持
